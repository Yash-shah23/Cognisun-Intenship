# ğŸ¤– Simple AI Agent using LangChain + Ollama (Gemma 2B) + RAG

This is a minimal, local **Retrieval-Augmented Generation (RAG)** agent built with LangChain, FAISS, HuggingFace Embeddings, and the **Gemma 2B LLM** running via **Ollama**.

Ask natural-language questions about a plain text file (`docs.txt`) â€” answers are generated based on context retrieved from that file.

---

## ğŸ“ Project Structure
project-root/
â”‚
â”œâ”€â”€ main.py # FastAPI app with POST endpoint /ask
â”œâ”€â”€ rag_chain.py # Loads, splits, embeds text + builds RAG pipeline
â”œâ”€â”€ docs.txt # Your input knowledge file
â”œâ”€â”€ requirements.txt # Python dependencies

---

## ğŸ§  What It Does

- Loads `docs.txt` (your knowledge base)
- Splits it into chunks
- Embeds using HuggingFace (`MiniLM`)
- Indexes with FAISS
- Queries via Ollama running `gemma:2b`

---

## âš™ï¸ Requirements

Install the following:

- Python 3.10+
- [Ollama](https://ollama.com/) (for running `gemma`)
- `docs.txt` file (in root folder)

---

## ğŸ“¦ Install Dependencies

```bash
python -m venv venv
source venv/bin/activate  # or venv\Scripts\activate on Windows

pip install -r requirements.txt

Requirements.txt

fastapi
uvicorn
pydantic
langchain
langchain-community
sentence-transformers


ğŸ§  Step 1: Start Ollama
You must run the Gemma model via Ollama before using the app:

ollama run gemma:2b

ğŸ› ï¸ Step 2: Run the App

uvicorn main:app --reload

ğŸ“ Example docs.txt

LangChain is an open-source framework for developing applications powered by language models. It enables easy integration with LLMs, vector databases, and retrieval systems like FAISS.

ğŸ‘¨â€ğŸ’» Author

Built with â¤ï¸ by [Yash Shah]

