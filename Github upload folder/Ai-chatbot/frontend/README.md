# ğŸ¤– AI Chatbot with RAG (FastAPI + FAISS + React)

This is a **simple AI-powered chatbot** built using:

- ğŸ§  **Retrieval-Augmented Generation (RAG)**
- âš™ï¸ **FastAPI** for the backend
- ğŸ’¾ **FAISS** vector search
- ğŸ§¬ **HuggingFace Embeddings**
- ğŸ’¬ **Ollama (Llama3)** as the LLM
- ğŸ¨ **React** for a ChatGPT-like frontend UI

---

## ğŸ“‚ Project Structure


Ai-chatbot/
â”œâ”€â”€ backend/
â”‚ â”œâ”€â”€ main.py # FastAPI app
â”‚ â”œâ”€â”€ ingest.py # RAG chain with FAISS + LLM
â”‚ â”œâ”€â”€ Test.pdf # Source document for QA
â”‚ â”œâ”€â”€requirements.txt
â”‚
â”œâ”€â”€ frontend/
â”‚ â”œâ”€â”€ src/
â”‚ â”‚ â”œâ”€â”€ App.jsx # Chat UI logic
â”‚ â”‚ â”œâ”€â”€ App.css # ChatGPT-style UI
â”‚ â”‚ â””â”€â”€ index.js
â”‚ â””â”€â”€ package.json
â””â”€â”€ README.md


---

## ğŸ› ï¸ Installation

### ğŸ”§ Backend Setup (FastAPI)

1. Navigate to the backend folder:

    cd backend

2. Create and activate a virtual environment:

    python -m venv venv
    venv\Scripts\activate   # Windows

3. Install all dependencies:

    pip install -r requirements.txt

4. Start the FastAPI server:

    uvicorn main:app --reload

â¡ï¸ Server runs at: http://localhost:8000


ğŸ’» Frontend Setup (React)
    

1. Exit the backend folder:

    cd ..

2. Make react project

    npm create-react-app frontend 

3. Install React dependencies:

    npm install 

4.  npm start

    npm start 
        # Starts the frontend server 

â¡ï¸ App runs at: http://localhost:3000


ğŸ” How It Works:

    A PDF document (e.g., Test.pdf) is embedded using HuggingFace embeddings.

    FAISS indexes and stores chunks of the document.

    When a user asks a question:

    Relevant chunks are retrieved by FAISS.

    These chunks + the question are passed to Ollama LLM (Llama3).

    The LLM generates an answer based only on the retrieved content.

    If the question is unrelated to the document, it replies with:
    "Provided question is out of context."


âœ… This is called RAG (Retrieval-Augmented Generation).

ğŸ§ª Test Cases

âœ… Valid Questions (From PDF)

Question-->

    Q1.What was Amazon's total revenue in 2023?	
    Q2.What is Project Kuiper?	
    Q3.How much did AWS grow year-over-year?	
    Q4.What are primitives in AWS?	 

âŒ Out-of-Context Questions

Question-->	

    Q1.What is quantum physics?	
    "Provided question is out of context."
    
    Q2.Who is Elon Musk?	
    "Provided question is out of context."

    Q3.What is the capital of Canada?	
    "Provided question is out of context."

ğŸ’¡ LLM Behavior

    Model Used: gemma2:9b-instruct-q4_0 via Ollama

    Friendly questions like "hello" or "how are you?" are answered directly by the LLM (fallback when no context is retrieved).

You can change the model by editing this line in rag_engine.py:

   """ OllamaLLM(model="gemma2:9b-instruct-q4_0") """


ğŸš€ Features
    âœ… RAG-based accurate answers

    âœ… Friendly responses for casual messages

    âœ… "Out of context" detection

    âœ… Fast and responsive frontend/backend


ğŸ§± Technologies Used

    Component	Tech Stack
    Backend	FastAPI, LangChain, FAISS
    Embeddings	HuggingFace MiniLM
    LLM	Ollama (gemma2:9b-instruct-q4_0)
    Frontend	React + Axios + CSS


ğŸ—ƒï¸ Requirements

    Python 3.10+

    Node.js 16+

    Ollama installed and model pulled (ollama pull gemma2:9b-instruct-q4_0)

    Chrome or modern browser


React

ğŸ“Œ Future Improvements (Optional Ideas)

    Upload new files dynamically

    Support multilingual documents

    Use chat history to give better context

    Switch between different models (Mistral, Gemma, GPT-4 etc.)


ğŸ§  What You Learn from This Project:

    ğŸ” How Retrieval-Augmented Generation (RAG) works

    ğŸ§¬ How to use HuggingFace embeddings + FAISS

    âš™ï¸ How to connect FastAPI with LangChain and Ollama



